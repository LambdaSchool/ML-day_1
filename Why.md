Why do you want to learn ML?

I am particularly interested in ML because, in college, I studied learning in humans and animals. Specifically, I was trained in behavior analysis and we often had to conduct tedious experiments that required a large amount of sifting through data (i.e., video data, gait analysis, etc.). With ML, classifiers could be constructed to analyze hours of video of subjects and record certain behaviors for instance. For humans, this takes a considerable bit of time. In all, I want to study ML in the hopes of allowing experimenters, specifically in psychology and neuroscience, to accelerate their studies by minimizing the amount of time it takes sift through raw data. This becomes extremely important when dealing with something as complex as the brain. If one day the tech becomes available to where we could analyze the firing of each neuron in a specific area of the brain, we would need quite a robust algorithm to be able to analyze that much data for so many different points of reference.

ML is exciting simply because it has the potential to help improve our understanding and assist us (hopefully) in being able to focus on more elevated goals instead of mundane calculations. Could the great scientists of the past made greater strives if they had help in analyzing their data allowing them to focus more on what it all meant? Who knows. But, it is intriguing to wonder.

To be employable, I would assume you would need basic CS skills, definitely probability and statistics, know how to model data, proficiency at ML algorithms and libraries, and knowledge of software engineering. Python, Perl, Java, or C++ might be languages to know. Maybe some UX design experience as well if one would be working with ML that is specifically made to interface and adapt with user input.

I think the most current danger to studying ML is that it often must learn by studying the user, in depth. So, often that means that much information must be accumulated from the user which obviously raises the alarms for breaches of privacy. Furthermore, the algorithm will actually know the users preferences, behaviors, and be able to manipulate what that user sees. If it is hacked, ML could be an intense weapon is swaying public opinion, let's say, by gently gnudging that person from their current state. However, ML suffers from what all cutting edge science suffers from and that is the inability to predict how certain advancements will be used or altered for malicious purposes. 

I do think that scientists and inventors should be conscious of predicting as many potential threats to the integrity of their design and combatting any incursions before they happen. Obviously, many places do this already, but it is even more crucial in a field where we are creating potential entities that know everything about us, can predict our behavior, and, in many ways even now, control our behavior. It is an interesting thought experiment to be sure. But, regulation should be put in place just like with many other scientific endeavors. Maybe a machine learning algorithm that can run the outcome of another ML and lay out the probability of how it will adapt in the future. Should we learn how to predict how machines learn before we train machines how to predict our learning?

This might have been too long of an answer, but thank you for reading if you made it this far. :)